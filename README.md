# Post-training Reading Group - Mountain View

A monthly reading group focused on the latest research in post-training techniques for large language models, including RFT, RLHF, preference learning, synthetic preference data, and related topics. 

## ðŸ“š Papers by Category

- [Aug 7, 2025] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/pdf/2507.01352) (Liu et al., 2025)

## ðŸ“… Meeting Information

- **Frequency**: Monthly
- **Location**: Collinear HQ in Mountain View, CA
- **Format**: In-person discussion of selected papers
- **Duration**: 1-2 hours per session

## ðŸŽ¯ Focus Areas

Our reading group covers various aspects of post-training research:

- **Reinforcement Learning from Human Feedback (RLHF)**
- **Direct Preference Optimization (DPO) and variants**
- **Preference learning and reward modeling**
- **Alignment and safety techniques**
- **Evaluation and benchmarking**
- **Human-AI collaboration in preference data**

## âœ‹ How to Participate

- Sign up for our [mailing list](https://docs.google.com/forms/d/e/1FAIpQLSfKtuySdkMVQbR1PLZ4c-vCHm2rzbUmZAwyoThcgkxN1ifMRg/viewform)
- Show up to our [events](https://lu.ma/event/manage/evt-d5yvXaoyYSgNgkn) 
- Suggest papers for discussion
- Co-organize sessions

## ðŸ“ž Contact

For questions about the reading group or to suggest papers, please open an issue in this repository or contact research@collinear.ai

---

*Last updated: July 2025*

